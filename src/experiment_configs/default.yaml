# 기본 실험 파라미터
base_params:
  dataset: "searchsnippets"
  n_clusters: null  # 데이터셋의 실제 클러스터 수 사용
  skip_gae: false  # GAE 학습 스킵 여부
  embeddings:
    tfidf:
      method: "threshold"
      max_features: null
      threshold: 0.9
    pretrained:
      text_model: "BAAI/bge-large-en-v1.5"
      text_model_type: "sentence_transformer"
      image_model: "google/vit-base-patch16-224-in21k"
  graph:
    method: "reranked_tfidf_text"
    alpha: 0.2
    max_connections: 20

# 실험할 하이퍼파라미터 조합
experiments:
  gae:
    enabled: true
    params:
      hidden_dim: [128, 256]
      out_dim: [64, 128]
      learning_rate: [0.001, 0.01, 0.1]
      init_feature: ["onehot", "sparse", "dense", "combined"]
    fixed_params:
      epochs: 1000
      eval_interval: 10
      early_stopping: 100

  proposed:
    enabled: true
    params:
      channels1: [128, 256]
      channels2: [64, 128]
      lr: [0.001, 0.01, 0.1]
      lambda_factor: [0.5, 1.0]
      hard_negative_ratio: [0.0, 0.3, 0.5]
      k_hop_values: [[3, 4], [4, 5]]
    fixed_params:
      epochs: 1000
      eval_interval: 10
      early_stopping: 100
      use_scheduler: true
      use_standard_gae: false
      gae_init_feature: "combined"
      use_k_hop_negatives: true

  graph:
    enabled: true
    methods: ["tfidf", "pretrained_text", "reranked_tfidf_text"]
    params:
      alpha: [0.1, 0.2, 0.3, 0.4, 0.5]  # reranked_tfidf_text에서만 사용
      max_connections: [10, 20, 30, 40, 50]

  embeddings:
    enabled: true
    tfidf:
      methods: ["standard", "threshold"]
      thresholds: [0.8, 0.85, 0.9, 0.95]  # threshold 방식에서만 사용
    pretrained_models:
      - "BAAI/bge-large-en-v1.5"
      - "sentence-transformers/all-MiniLM-L6-v2"
      - "sentence-transformers/all-mpnet-base-v2" 